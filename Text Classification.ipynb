{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
=======
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from dalab import read_pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, SimpleRNN, GRU, LSTM\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>17410</th>\n",
       "      <td>christian</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!rochester!rutg...</td>\n",
=======
<<<<<<< HEAD
       "      <th>8373</th>\n",
       "      <td>artificial_intelligence</td>\n",
       "      <td>it has become a sort of tradition for me to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7822</th>\n",
       "      <td>transfer_learning</td>\n",
       "      <td>i’m building style transfer applications for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>computer_vision</td>\n",
       "      <td>init27 is an initiative by deep learning and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>data_science</td>\n",
       "      <td>data science for social good is now four year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>data_mining</td>\n",
       "      <td>data mining is the computational process of d...</td>\n",
=======
       "      <th>18872</th>\n",
       "      <td>forsale</td>\n",
       "      <td>Newsgroups: misc.forsale\\nPath: cantaloupe.srv...</td>\n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>baseball</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9607</th>\n",
       "      <td>misc</td>\n",
       "      <td>Newsgroups: talk.politics.misc\\nPath: cantalou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>hardware</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!rochester!udel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>8281</th>\n",
       "      <td>med</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu misc.consumers...</td>\n",
=======
       "      <th>14644</th>\n",
       "      <td>electronics</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "           label                                               text\n",
       "17410  christian  Path: cantaloupe.srv.cs.cmu.edu!rochester!rutg...\n",
       "4586    baseball  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "9607        misc  Newsgroups: talk.politics.misc\\nPath: cantalou...\n",
       "2043    hardware  Path: cantaloupe.srv.cs.cmu.edu!rochester!udel...\n",
       "8281         med  Xref: cantaloupe.srv.cs.cmu.edu misc.consumers..."
=======
<<<<<<< HEAD
       "                         class  \\\n",
       "8373   artificial_intelligence   \n",
       "7822         transfer_learning   \n",
       "2250           computer_vision   \n",
       "10002             data_science   \n",
       "5222               data_mining   \n",
       "\n",
       "                                                   story  \n",
       "8373    it has become a sort of tradition for me to t...  \n",
       "7822    i’m building style transfer applications for ...  \n",
       "2250    init27 is an initiative by deep learning and ...  \n",
       "10002   data science for social good is now four year...  \n",
       "5222    data mining is the computational process of d...  "
=======
       "             label                                               text\n",
       "18872      forsale  Newsgroups: misc.forsale\\nPath: cantaloupe.srv...\n",
       "6666        hockey  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
       "15811         guns  Newsgroups: talk.politics.guns\\nPath: cantalou...\n",
       "7404         crypt  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club...\n",
       "14644  electronics  Path: cantaloupe.srv.cs.cmu.edu!magnesium.club..."
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# df = pd.read_pickle('data/medium_stories/dataframes/en_lower_stories.pickle').reset_index(drop=True)\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.drop_duplicates(subset='story')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12268</th>\n",
       "      <td>graphics</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19175</th>\n",
       "      <td>misc</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.magick:992...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12385</th>\n",
       "      <td>graphics</td>\n",
       "      <td>Newsgroups: comp.graphics\\nPath: cantaloupe.sr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18956</th>\n",
       "      <td>misc</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.magick:100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12519</th>\n",
       "      <td>graphics</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.binaries.p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text\n",
       "12268  graphics  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "19175      misc  Xref: cantaloupe.srv.cs.cmu.edu alt.magick:992...\n",
       "12385  graphics  Newsgroups: comp.graphics\\nPath: cantaloupe.sr...\n",
       "18956      misc  Xref: cantaloupe.srv.cs.cmu.edu alt.magick:100...\n",
       "12519  graphics  Xref: cantaloupe.srv.cs.cmu.edu alt.binaries.p..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
    "df = read_pickle('data/20_newsgroup/dataframes/raw_news.pickle')\n",
    "df = df.sample(frac=1)\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": null,
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('data/medium_stories/dataframes/en_lower_stories.pickle').reset_index(drop=True)\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.drop_duplicates(subset='story')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
<<<<<<< HEAD
      "graphics 980\n",
      "misc 2704\n",
      "crypt 998\n",
      "hardware 1968\n",
      "forsale 990\n",
      "mideast 974\n",
      "x 992\n",
      "hockey 994\n",
      "motorcycles 1000\n",
      "autos 990\n",
      "med 997\n",
      "atheism 886\n",
      "guns 952\n",
      "electronics 987\n",
      "baseball 993\n",
      "christian 997\n",
      "space 991\n"
=======
      "forsale 990\n",
      "hockey 994\n",
      "guns 964\n",
      "crypt 998\n",
      "electronics 988\n",
      "hardware 1966\n",
      "x 992\n",
      "motorcycles 1000\n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
      "christian 997\n",
      "baseball 993\n",
      "misc 2681\n",
<<<<<<< HEAD
      "hardware 1965\n",
      "med 994\n",
      "x 991\n",
      "electronics 989\n",
      "guns 964\n",
      "space 991\n",
      "motorcycles 999\n",
      "mideast 973\n",
      "graphics 987\n",
      "forsale 992\n",
      "autos 990\n",
      "crypt 999\n",
      "hockey 994\n",
      "atheism 894\n"
=======
      "mideast 980\n",
      "med 992\n",
      "graphics 985\n",
      "space 993\n"
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "for label in df.label.unique():\n",
    "    print(label, len(df[df.label == label]))"
=======
    "for topic in df['label'].unique():\n",
    "    print(topic, len(df[df['label'] == topic]))"
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 5,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_multiple_labels(df, column, limit):\n",
    "    for topic in df[column].unique():\n",
    "        sample = df[df[column] == topic]\n",
    "        if len(sample) < limit:\n",
    "            df = df.drop(sample.index)\n",
    "\n",
    "    for topic in df[column].unique():\n",
    "        sample = df[df[column] == topic]\n",
    "        if len(sample) > limit:\n",
    "            df = df.drop(sample.index)\n",
    "            new_sample = sample.sample(limit)\n",
    "            df = df.append(new_sample)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = balance_multiple_labels(df, 'label', 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXLEN = 1000\n",
    "VOCAB_SIZE = 20000\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 6,
   "metadata": {},
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 8,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(' '.join(df.text.tolist()))\n",
    "word_counts = Counter(all_words).most_common(VOCAB_SIZE)\n",
    "words = [w[0] for w in word_counts]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 9,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [01:30, 220.35it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_dic = {}\n",
    "for index, word in tqdm(enumerate(words)):\n",
    "    token = nlp(word)\n",
    "    embed_dic[token.text] = token.vector"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": 10,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>&gt;</th>\n",
       "      <td>3.271235</td>\n",
       "      <td>0.865377</td>\n",
       "      <td>-2.062254</td>\n",
       "      <td>1.412337</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>1.570737</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>-1.616881</td>\n",
       "      <td>-3.192619</td>\n",
       "      <td>-2.547622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612776</td>\n",
       "      <td>-0.346440</td>\n",
       "      <td>4.841980</td>\n",
       "      <td>0.257593</td>\n",
       "      <td>-1.508441</td>\n",
       "      <td>5.375728</td>\n",
       "      <td>1.134497</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>2.945163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>3.070288</td>\n",
       "      <td>0.986614</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>1.810464</td>\n",
       "      <td>2.261192</td>\n",
       "      <td>-0.716290</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>-1.720148</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>-3.444734</td>\n",
=======
       "      <th>\u0002</th>\n",
<<<<<<< HEAD
       "      <td>-0.917690</td>\n",
       "      <td>1.594071</td>\n",
=======
       "      <td>-0.917688</td>\n",
       "      <td>1.594070</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>6.749008</td>\n",
       "      <td>0.536356</td>\n",
       "      <td>-1.675298</td>\n",
       "      <td>3.894897</td>\n",
       "      <td>-3.105777</td>\n",
<<<<<<< HEAD
       "      <td>2.663858</td>\n",
       "      <td>1.218638</td>\n",
       "      <td>-1.035808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230747</td>\n",
=======
       "      <td>2.663857</td>\n",
       "      <td>1.218641</td>\n",
       "      <td>-1.035807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230748</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>0.118247</td>\n",
       "      <td>-0.219932</td>\n",
       "      <td>0.352065</td>\n",
       "      <td>-0.254422</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-1.410054</td>\n",
<<<<<<< HEAD
       "      <td>0.657523</td>\n",
=======
       "      <td>0.657522</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>-0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.671140</td>\n",
       "      <td>-0.475780</td>\n",
       "      <td>1.225881</td>\n",
       "      <td>-0.533356</td>\n",
       "      <td>1.413613</td>\n",
       "      <td>2.528171</td>\n",
       "      <td>-0.030113</td>\n",
       "      <td>0.486535</td>\n",
       "      <td>3.412096</td>\n",
       "      <td>1.299004</td>\n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "      <td>...</td>\n",
       "      <td>-3.068707</td>\n",
       "      <td>0.864790</td>\n",
       "      <td>3.221823</td>\n",
       "      <td>1.004827</td>\n",
       "      <td>-0.291131</td>\n",
       "      <td>6.993183</td>\n",
       "      <td>-0.521152</td>\n",
       "      <td>-2.925200</td>\n",
       "      <td>-0.208749</td>\n",
       "      <td>2.764246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>,</th>\n",
       "      <td>0.435715</td>\n",
       "      <td>2.059121</td>\n",
       "      <td>-2.827400</td>\n",
       "      <td>5.559331</td>\n",
       "      <td>-0.635631</td>\n",
       "      <td>-0.084876</td>\n",
       "      <td>1.348762</td>\n",
       "      <td>-1.338799</td>\n",
       "      <td>-2.545363</td>\n",
       "      <td>-4.161990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583077</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>4.302979</td>\n",
       "      <td>-2.965968</td>\n",
       "      <td>-0.829183</td>\n",
       "      <td>1.851194</td>\n",
       "      <td>4.129368</td>\n",
       "      <td>-2.011104</td>\n",
       "      <td>-1.491037</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.017206</td>\n",
       "      <td>1.433102</td>\n",
       "      <td>2.001210</td>\n",
       "      <td>3.140334</td>\n",
       "      <td>1.852441</td>\n",
       "      <td>-0.963332</td>\n",
       "      <td>1.899922</td>\n",
       "      <td>-0.401818</td>\n",
       "      <td>-2.946477</td>\n",
       "      <td>-2.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.228596</td>\n",
       "      <td>0.986874</td>\n",
       "      <td>1.485204</td>\n",
       "      <td>-2.851583</td>\n",
       "      <td>-2.198703</td>\n",
       "      <td>2.439004</td>\n",
       "      <td>3.210545</td>\n",
       "      <td>-2.390744</td>\n",
       "      <td>-1.637757</td>\n",
       "      <td>0.658658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.438195</td>\n",
       "      <td>-0.151792</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>2.482028</td>\n",
       "      <td>0.040842</td>\n",
       "      <td>0.089026</td>\n",
       "      <td>2.102615</td>\n",
       "      <td>-1.106307</td>\n",
       "      <td>-3.831193</td>\n",
       "      <td>-2.613150</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.212205</td>\n",
       "      <td>0.977764</td>\n",
       "      <td>2.540429</td>\n",
       "      <td>-1.793072</td>\n",
       "      <td>-1.837151</td>\n",
       "      <td>3.268545</td>\n",
       "      <td>3.442772</td>\n",
       "      <td>-2.944798</td>\n",
       "      <td>-1.267171</td>\n",
       "      <td>0.232743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-1.553254</td>\n",
       "      <td>-2.913822</td>\n",
       "      <td>-4.319618</td>\n",
       "      <td>2.272106</td>\n",
       "      <td>-1.109262</td>\n",
       "      <td>1.648155</td>\n",
       "      <td>-2.616732</td>\n",
       "      <td>-2.518888</td>\n",
       "      <td>-2.483821</td>\n",
       "      <td>-2.783311</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250357</td>\n",
       "      <td>-1.727508</td>\n",
       "      <td>4.852344</td>\n",
       "      <td>-0.260655</td>\n",
       "      <td>3.556051</td>\n",
       "      <td>6.408743</td>\n",
       "      <td>-0.459191</td>\n",
       "      <td>1.640075</td>\n",
       "      <td>-0.309467</td>\n",
       "      <td>-1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>-0.836566</td>\n",
       "      <td>1.029746</td>\n",
       "      <td>-1.545806</td>\n",
       "      <td>0.859156</td>\n",
       "      <td>3.777660</td>\n",
       "      <td>-0.975338</td>\n",
       "      <td>0.222508</td>\n",
       "      <td>-0.303487</td>\n",
       "      <td>-1.498767</td>\n",
       "      <td>-2.576878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.571487</td>\n",
       "      <td>1.666716</td>\n",
       "      <td>3.351637</td>\n",
       "      <td>1.144979</td>\n",
       "      <td>-1.691302</td>\n",
       "      <td>5.329454</td>\n",
       "      <td>0.757731</td>\n",
       "      <td>-3.216542</td>\n",
       "      <td>-1.397676</td>\n",
       "      <td>2.549656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@</th>\n",
       "      <td>1.050108</td>\n",
       "      <td>0.208937</td>\n",
       "      <td>0.347154</td>\n",
       "      <td>2.047950</td>\n",
       "      <td>1.639953</td>\n",
       "      <td>1.493342</td>\n",
       "      <td>1.208333</td>\n",
       "      <td>0.460561</td>\n",
       "      <td>-2.761452</td>\n",
       "      <td>-2.527010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653786</td>\n",
       "      <td>0.561741</td>\n",
       "      <td>2.965085</td>\n",
       "      <td>1.466421</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>3.923660</td>\n",
       "      <td>4.240563</td>\n",
       "      <td>0.821820</td>\n",
       "      <td>-1.435084</td>\n",
       "      <td>3.934670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.061328</td>\n",
       "      <td>1.449228</td>\n",
       "      <td>-3.866241</td>\n",
       "      <td>0.745157</td>\n",
       "      <td>-1.463142</td>\n",
       "      <td>0.375383</td>\n",
       "      <td>-2.931649</td>\n",
       "      <td>-0.653668</td>\n",
       "      <td>-2.569345</td>\n",
       "      <td>-3.578854</td>\n",
=======
       "      <th>#</th>\n",
       "      <td>1.499199</td>\n",
       "      <td>-0.151667</td>\n",
       "      <td>2.150064</td>\n",
       "      <td>1.835209</td>\n",
       "      <td>1.904099</td>\n",
       "      <td>2.142190</td>\n",
       "      <td>-1.108657</td>\n",
       "      <td>-1.281632</td>\n",
       "      <td>2.732129</td>\n",
       "      <td>2.948514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079698</td>\n",
       "      <td>-0.464940</td>\n",
       "      <td>1.290173</td>\n",
       "      <td>0.061074</td>\n",
       "      <td>-0.257399</td>\n",
       "      <td>-0.752442</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.132082</td>\n",
       "      <td>-0.440150</td>\n",
       "      <td>-0.476224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.630779</td>\n",
       "      <td>1.138583</td>\n",
       "      <td>2.530837</td>\n",
       "      <td>0.166182</td>\n",
       "      <td>3.076834</td>\n",
       "      <td>0.542186</td>\n",
       "      <td>-0.858886</td>\n",
       "      <td>0.884038</td>\n",
       "      <td>2.754835</td>\n",
       "      <td>-0.390934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297402</td>\n",
       "      <td>-0.254304</td>\n",
       "      <td>1.426802</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.657113</td>\n",
       "      <td>-0.627470</td>\n",
       "      <td>0.097198</td>\n",
       "      <td>-0.183204</td>\n",
       "      <td>-0.213610</td>\n",
       "      <td>-0.170229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%</th>\n",
       "      <td>2.040905</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>2.365522</td>\n",
       "      <td>-1.138491</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>2.351219</td>\n",
       "      <td>-2.068765</td>\n",
       "      <td>-0.857941</td>\n",
       "      <td>0.967327</td>\n",
       "      <td>2.126301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.527834</td>\n",
       "      <td>-0.229152</td>\n",
       "      <td>-0.059828</td>\n",
       "      <td>0.299519</td>\n",
       "      <td>-0.925736</td>\n",
       "      <td>-0.175775</td>\n",
       "      <td>0.280793</td>\n",
       "      <td>0.260768</td>\n",
       "      <td>0.674299</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;</th>\n",
       "      <td>-0.362177</td>\n",
       "      <td>-1.536422</td>\n",
       "      <td>0.681591</td>\n",
       "      <td>-0.254282</td>\n",
       "      <td>-0.020796</td>\n",
       "      <td>2.549081</td>\n",
       "      <td>1.063519</td>\n",
       "      <td>1.306450</td>\n",
       "      <td>1.050380</td>\n",
       "      <td>2.485575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431889</td>\n",
       "      <td>-0.154748</td>\n",
       "      <td>-0.647066</td>\n",
       "      <td>-0.048509</td>\n",
       "      <td>0.023910</td>\n",
       "      <td>-0.560397</td>\n",
       "      <td>0.427393</td>\n",
       "      <td>0.642400</td>\n",
       "      <td>0.882393</td>\n",
       "      <td>-0.388471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-1.620778</td>\n",
       "      <td>2.052795</td>\n",
       "      <td>0.476202</td>\n",
       "      <td>-0.315579</td>\n",
       "      <td>0.532584</td>\n",
       "      <td>-0.451268</td>\n",
       "      <td>-1.238636</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>-0.797014</td>\n",
       "      <td>0.126661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054849</td>\n",
       "      <td>-0.057004</td>\n",
       "      <td>0.090347</td>\n",
       "      <td>-0.235630</td>\n",
       "      <td>-0.785747</td>\n",
       "      <td>-0.306805</td>\n",
       "      <td>0.576140</td>\n",
       "      <td>0.273453</td>\n",
       "      <td>0.878228</td>\n",
       "      <td>0.013317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>-1.888850</td>\n",
       "      <td>-0.329436</td>\n",
       "      <td>2.229829</td>\n",
       "      <td>-0.024573</td>\n",
       "      <td>0.612867</td>\n",
       "      <td>1.830824</td>\n",
       "      <td>-2.658098</td>\n",
       "      <td>1.066224</td>\n",
       "      <td>-0.894128</td>\n",
       "      <td>0.677598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093398</td>\n",
       "      <td>-0.030841</td>\n",
       "      <td>-0.173364</td>\n",
       "      <td>-0.138874</td>\n",
       "      <td>-0.683034</td>\n",
       "      <td>-0.094501</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.371119</td>\n",
       "      <td>0.759352</td>\n",
       "      <td>0.149495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'*</th>\n",
<<<<<<< HEAD
       "      <td>1.111743</td>\n",
       "      <td>1.290677</td>\n",
       "      <td>-0.382141</td>\n",
       "      <td>0.069084</td>\n",
=======
       "      <td>1.111745</td>\n",
       "      <td>1.290676</td>\n",
       "      <td>-0.382141</td>\n",
       "      <td>0.069083</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>-0.239147</td>\n",
       "      <td>0.952505</td>\n",
       "      <td>-1.702639</td>\n",
       "      <td>1.894596</td>\n",
       "      <td>1.396065</td>\n",
       "      <td>0.785170</td>\n",
<<<<<<< HEAD
       "      <td>...</td>\n",
       "      <td>-0.027173</td>\n",
       "      <td>0.091146</td>\n",
       "      <td>-0.310856</td>\n",
       "      <td>-0.363376</td>\n",
       "      <td>-0.392699</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>-0.072186</td>\n",
       "      <td>0.011815</td>\n",
       "      <td>0.666233</td>\n",
       "      <td>-0.253421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'+</th>\n",
       "      <td>0.697192</td>\n",
       "      <td>0.727430</td>\n",
       "      <td>0.795637</td>\n",
       "      <td>0.136264</td>\n",
       "      <td>0.777219</td>\n",
       "      <td>0.521606</td>\n",
       "      <td>-0.411749</td>\n",
       "      <td>0.238003</td>\n",
       "      <td>-0.009106</td>\n",
       "      <td>0.776705</td>\n",
       "      <td>...</td>\n",
=======
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "      <td>...</td>\n",
       "      <td>-0.515135</td>\n",
       "      <td>2.122284</td>\n",
       "      <td>1.689541</td>\n",
       "      <td>3.462939</td>\n",
       "      <td>1.066636</td>\n",
       "      <td>7.533019</td>\n",
       "      <td>2.215232</td>\n",
       "      <td>-2.120562</td>\n",
       "      <td>-2.444374</td>\n",
       "      <td>2.538816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.479581</td>\n",
       "      <td>0.744241</td>\n",
       "      <td>-0.826027</td>\n",
       "      <td>0.408746</td>\n",
       "      <td>1.193886</td>\n",
       "      <td>-1.085895</td>\n",
       "      <td>-0.922773</td>\n",
       "      <td>-0.325466</td>\n",
       "      <td>-1.257374</td>\n",
       "      <td>-5.033413</td>\n",
       "      <td>...</td>\n",
<<<<<<< HEAD
       "      <td>1.058636</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>3.694527</td>\n",
       "      <td>-1.901443</td>\n",
       "      <td>-2.631643</td>\n",
       "      <td>4.643446</td>\n",
       "      <td>3.615112</td>\n",
       "      <td>-3.458215</td>\n",
       "      <td>0.200469</td>\n",
       "      <td>3.789077</td>\n",
=======
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>0.075038</td>\n",
       "      <td>0.048530</td>\n",
       "      <td>0.151208</td>\n",
       "      <td>-0.007558</td>\n",
       "      <td>-0.555188</td>\n",
       "      <td>-0.124780</td>\n",
       "      <td>0.504512</td>\n",
       "      <td>0.432797</td>\n",
       "      <td>1.108591</td>\n",
<<<<<<< HEAD
       "      <td>0.201012</td>\n",
=======
       "      <td>0.201013</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "           0         1         2         3         4         5         6   \\\n",
       ">    3.271235  0.865377 -2.062254  1.412337 -0.318887  1.570737  0.218967   \n",
       ":    3.070288  0.986614 -1.164513  1.810464  2.261192 -0.716290  0.249038   \n",
       ",    0.435715  2.059121 -2.827400  5.559331 -0.635631 -0.084876  1.348762   \n",
       ".    0.017206  1.433102  2.001210  3.140334  1.852441 -0.963332  1.899922   \n",
       "!   -0.438195 -0.151792  0.629396  2.482028  0.040842  0.089026  2.102615   \n",
       "the -1.553254 -2.913822 -4.319618  2.272106 -1.109262  1.648155 -2.616732   \n",
       "--  -0.836566  1.029746 -1.545806  0.859156  3.777660 -0.975338  0.222508   \n",
       "@    1.050108  0.208937  0.347154  2.047950  1.639953  1.493342  1.208333   \n",
       "to  -0.061328  1.449228 -3.866241  0.745157 -1.463142  0.375383 -2.931649   \n",
       ")    0.479581  0.744241 -0.826027  0.408746  1.193886 -1.085895 -0.922773   \n",
=======
       "         0         1         2         3         4         5         6    \\\n",
<<<<<<< HEAD
       "\u0002  -0.917690  1.594071  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!   0.671140 -0.475780  1.225881 -0.533356  1.413613  2.528171 -0.030113   \n",
       "#   1.499199 -0.151667  2.150064  1.835209  1.904099  2.142190 -1.108657   \n",
       "$   0.630779  1.138583  2.530837  0.166182  3.076834  0.542186 -0.858886   \n",
       "%   2.040905  0.173398  2.365522 -1.138491  0.034594  2.351219 -2.068765   \n",
       "&  -0.362177 -1.536422  0.681591 -0.254282 -0.020796  2.549081  1.063519   \n",
       "'  -1.620778  2.052795  0.476202 -0.315579  0.532584 -0.451268 -1.238636   \n",
       "'' -1.888850 -0.329436  2.229829 -0.024573  0.612867  1.830824 -2.658098   \n",
       "'*  1.111743  1.290677 -0.382141  0.069084 -0.239147  0.952505 -1.702639   \n",
       "'+  0.697192  0.727430  0.795637  0.136264  0.777219  0.521606 -0.411749   \n",
       "\n",
       "         7         8         9      ...          374       375       376  \\\n",
       "\u0002   2.663858  1.218638 -1.035808    ...     0.425276  1.230747  0.118247   \n",
       "!   0.486535  3.412096  1.299004    ...     0.361494  0.078374 -0.094767   \n",
       "#  -1.281632  2.732129  2.948514    ...     0.079698 -0.464940  1.290173   \n",
       "$   0.884038  2.754835 -0.390934    ...     0.297402 -0.254304  1.426802   \n",
       "%  -0.857941  0.967327  2.126301    ...    -0.527834 -0.229152 -0.059828   \n",
       "&   1.306450  1.050380  2.485575    ...    -0.431889 -0.154748 -0.647066   \n",
       "'   0.606206 -0.797014  0.126661    ...     0.054849 -0.057004  0.090347   \n",
       "''  1.066224 -0.894128  0.677598    ...    -0.093398 -0.030841 -0.173364   \n",
       "'*  1.894596  1.396065  0.785170    ...    -0.027173  0.091146 -0.310856   \n",
       "'+  0.238003 -0.009106  0.776705    ...     0.075038  0.048530  0.151208   \n",
       "\n",
       "         377       378       379       380       381       382       383  \n",
       "\u0002  -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657523 -0.292479  \n",
       "!  -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#   0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476224  \n",
       "$  -0.010306 -0.657113 -0.627470  0.097198 -0.183204 -0.213610 -0.170229  \n",
       "%   0.299519 -0.925736 -0.175775  0.280793  0.260768  0.674299  0.673200  \n",
       "&  -0.048509  0.023910 -0.560397  0.427393  0.642400  0.882393 -0.388471  \n",
       "'  -0.235630 -0.785747 -0.306805  0.576140  0.273453  0.878228  0.013317  \n",
       "'' -0.138874 -0.683034 -0.094501  0.465635  0.371119  0.759352  0.149495  \n",
       "'* -0.363376 -0.392699  0.008236 -0.072186  0.011815  0.666233 -0.253421  \n",
       "'+ -0.007558 -0.555188 -0.124780  0.504512  0.432797  1.108591  0.201012  \n",
=======
       "\u0002  -0.917688  1.594070  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!   0.671140 -0.475781  1.225882 -0.533356  1.413614  2.528172 -0.030113   \n",
       "#   1.499199 -0.151666  2.150062  1.835209  1.904099  2.142193 -1.108657   \n",
       "$   0.630779  1.138584  2.530838  0.166183  3.076835  0.542186 -0.858887   \n",
       "%   2.040906  0.173398  2.365521 -1.138491  0.034594  2.351219 -2.068765   \n",
       "&  -0.362176 -1.536422  0.681592 -0.254282 -0.020795  2.549080  1.063519   \n",
       "'  -1.620776  2.052795  0.476201 -0.315580  0.532586 -0.451270 -1.238636   \n",
       "'' -1.888850 -0.329437  2.229829 -0.024572  0.612867  1.830826 -2.658098   \n",
       "'*  1.111745  1.290676 -0.382141  0.069083 -0.239147  0.952505 -1.702639   \n",
       "'+  0.697193  0.727430  0.795636  0.136264  0.777219  0.521605 -0.411750   \n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "\n",
       "           7         8         9   ...        86        87        88  \\\n",
       ">   -1.616881 -3.192619 -2.547622  ...  0.612776 -0.346440  4.841980   \n",
       ":   -1.720148  0.056825 -3.444734  ... -3.068707  0.864790  3.221823   \n",
       ",   -1.338799 -2.545363 -4.161990  ... -0.583077  0.877851  4.302979   \n",
       ".   -0.401818 -2.946477 -2.282340  ... -1.228596  0.986874  1.485204   \n",
       "!   -1.106307 -3.831193 -2.613150  ... -2.212205  0.977764  2.540429   \n",
       "the -2.518888 -2.483821 -2.783311  ...  3.250357 -1.727508  4.852344   \n",
       "--  -0.303487 -1.498767 -2.576878  ... -2.571487  1.666716  3.351637   \n",
       "@    0.460561 -2.761452 -2.527010  ... -0.653786  0.561741  2.965085   \n",
       "to  -0.653668 -2.569345 -3.578854  ... -0.515135  2.122284  1.689541   \n",
       ")   -0.325466 -1.257374 -5.033413  ...  1.058636  2.931694  3.694527   \n",
       "\n",
<<<<<<< HEAD
       "           89        90        91        92        93        94        95  \n",
       ">    0.257593 -1.508441  5.375728  1.134497  0.064851 -0.028303  2.945163  \n",
       ":    1.004827 -0.291131  6.993183 -0.521152 -2.925200 -0.208749  2.764246  \n",
       ",   -2.965968 -0.829183  1.851194  4.129368 -2.011104 -1.491037  0.611928  \n",
       ".   -2.851583 -2.198703  2.439004  3.210545 -2.390744 -1.637757  0.658658  \n",
       "!   -1.793072 -1.837151  3.268545  3.442772 -2.944798 -1.267171  0.232743  \n",
       "the -0.260655  3.556051  6.408743 -0.459191  1.640075 -0.309467 -1.231600  \n",
       "--   1.144979 -1.691302  5.329454  0.757731 -3.216542 -1.397676  2.549656  \n",
       "@    1.466421  0.470562  3.923660  4.240563  0.821820 -1.435084  3.934670  \n",
       "to   3.462939  1.066636  7.533019  2.215232 -2.120562 -2.444374  2.538816  \n",
       ")   -1.901443 -2.631643  4.643446  3.615112 -3.458215  0.200469  3.789077  \n",
=======
       "         377       378       379       380       381       382       383  \n",
       "\u0002  -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657522 -0.292479  \n",
       "!  -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#   0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476223  \n",
       "$  -0.010306 -0.657113 -0.627469  0.097199 -0.183204 -0.213610 -0.170229  \n",
       "%   0.299519 -0.925737 -0.175775  0.280792  0.260768  0.674299  0.673200  \n",
       "&  -0.048509  0.023910 -0.560396  0.427393  0.642400  0.882393 -0.388471  \n",
       "'  -0.235629 -0.785747 -0.306805  0.576140  0.273453  0.878228  0.013317  \n",
       "'' -0.138874 -0.683034 -0.094501  0.465635  0.371119  0.759352  0.149494  \n",
       "'* -0.363376 -0.392699  0.008236 -0.072186  0.011815  0.666233 -0.253421  \n",
       "'+ -0.007558 -0.555188 -0.124780  0.504512  0.432797  1.108591  0.201013  \n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "\n",
       "[10 rows x 96 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 9,
=======
<<<<<<< HEAD
     "execution_count": 31,
=======
     "execution_count": 10,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_words = pd.DataFrame(embed_dic).T\n",
    "embed_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 11,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>&gt;</th>\n",
       "      <td>3.271235</td>\n",
       "      <td>0.865377</td>\n",
       "      <td>-2.062254</td>\n",
       "      <td>1.412337</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>1.570737</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>-1.616881</td>\n",
       "      <td>-3.192619</td>\n",
       "      <td>-2.547622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612776</td>\n",
       "      <td>-0.346440</td>\n",
       "      <td>4.841980</td>\n",
       "      <td>0.257593</td>\n",
       "      <td>-1.508441</td>\n",
       "      <td>5.375728</td>\n",
       "      <td>1.134497</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>2.945163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>3.070288</td>\n",
       "      <td>0.986614</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>1.810464</td>\n",
       "      <td>2.261192</td>\n",
       "      <td>-0.716290</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>-1.720148</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>-3.444734</td>\n",
=======
       "      <th>\u0002</th>\n",
<<<<<<< HEAD
       "      <td>-0.917690</td>\n",
       "      <td>1.594071</td>\n",
=======
       "      <td>-0.917688</td>\n",
       "      <td>1.594070</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>6.749008</td>\n",
       "      <td>0.536356</td>\n",
       "      <td>-1.675298</td>\n",
       "      <td>3.894897</td>\n",
       "      <td>-3.105777</td>\n",
<<<<<<< HEAD
       "      <td>2.663858</td>\n",
       "      <td>1.218638</td>\n",
       "      <td>-1.035808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230747</td>\n",
=======
       "      <td>2.663857</td>\n",
       "      <td>1.218641</td>\n",
       "      <td>-1.035807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425276</td>\n",
       "      <td>1.230748</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>0.118247</td>\n",
       "      <td>-0.219932</td>\n",
       "      <td>0.352065</td>\n",
       "      <td>-0.254422</td>\n",
       "      <td>-0.038656</td>\n",
       "      <td>-1.410054</td>\n",
<<<<<<< HEAD
       "      <td>0.657523</td>\n",
=======
       "      <td>0.657522</td>\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "      <td>-0.292479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.671140</td>\n",
       "      <td>-0.475780</td>\n",
       "      <td>1.225881</td>\n",
       "      <td>-0.533356</td>\n",
       "      <td>1.413613</td>\n",
       "      <td>2.528171</td>\n",
       "      <td>-0.030113</td>\n",
       "      <td>0.486535</td>\n",
       "      <td>3.412096</td>\n",
       "      <td>1.299004</td>\n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "      <td>...</td>\n",
       "      <td>-3.068707</td>\n",
       "      <td>0.864790</td>\n",
       "      <td>3.221823</td>\n",
       "      <td>1.004827</td>\n",
       "      <td>-0.291131</td>\n",
       "      <td>6.993183</td>\n",
       "      <td>-0.521152</td>\n",
       "      <td>-2.925200</td>\n",
       "      <td>-0.208749</td>\n",
       "      <td>2.764246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <th>,</th>\n",
       "      <td>0.435715</td>\n",
       "      <td>2.059121</td>\n",
       "      <td>-2.827400</td>\n",
       "      <td>5.559331</td>\n",
       "      <td>-0.635631</td>\n",
       "      <td>-0.084876</td>\n",
       "      <td>1.348762</td>\n",
       "      <td>-1.338799</td>\n",
       "      <td>-2.545363</td>\n",
       "      <td>-4.161990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583077</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>4.302979</td>\n",
       "      <td>-2.965968</td>\n",
       "      <td>-0.829183</td>\n",
       "      <td>1.851194</td>\n",
       "      <td>4.129368</td>\n",
       "      <td>-2.011104</td>\n",
       "      <td>-1.491037</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.017206</td>\n",
       "      <td>1.433102</td>\n",
       "      <td>2.001210</td>\n",
       "      <td>3.140334</td>\n",
       "      <td>1.852441</td>\n",
       "      <td>-0.963332</td>\n",
       "      <td>1.899922</td>\n",
       "      <td>-0.401818</td>\n",
       "      <td>-2.946477</td>\n",
       "      <td>-2.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.228596</td>\n",
       "      <td>0.986874</td>\n",
       "      <td>1.485204</td>\n",
       "      <td>-2.851583</td>\n",
       "      <td>-2.198703</td>\n",
       "      <td>2.439004</td>\n",
       "      <td>3.210545</td>\n",
       "      <td>-2.390744</td>\n",
       "      <td>-1.637757</td>\n",
       "      <td>0.658658</td>\n",
=======
       "      <th>#</th>\n",
       "      <td>1.499199</td>\n",
       "      <td>-0.151667</td>\n",
       "      <td>2.150064</td>\n",
       "      <td>1.835209</td>\n",
       "      <td>1.904099</td>\n",
       "      <td>2.142190</td>\n",
       "      <td>-1.108657</td>\n",
       "      <td>-1.281632</td>\n",
       "      <td>2.732129</td>\n",
       "      <td>2.948514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079698</td>\n",
       "      <td>-0.464940</td>\n",
       "      <td>1.290173</td>\n",
       "      <td>0.061074</td>\n",
       "      <td>-0.257399</td>\n",
       "      <td>-0.752442</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.132082</td>\n",
       "      <td>-0.440150</td>\n",
       "      <td>-0.476224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.630779</td>\n",
       "      <td>1.138583</td>\n",
       "      <td>2.530837</td>\n",
       "      <td>0.166182</td>\n",
       "      <td>3.076834</td>\n",
       "      <td>0.542186</td>\n",
       "      <td>-0.858886</td>\n",
       "      <td>0.884038</td>\n",
       "      <td>2.754835</td>\n",
       "      <td>-0.390934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297402</td>\n",
       "      <td>-0.254304</td>\n",
       "      <td>1.426802</td>\n",
       "      <td>-0.010306</td>\n",
       "      <td>-0.657113</td>\n",
       "      <td>-0.627470</td>\n",
       "      <td>0.097198</td>\n",
       "      <td>-0.183204</td>\n",
       "      <td>-0.213610</td>\n",
       "      <td>-0.170229</td>\n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
<<<<<<< HEAD
       ">      3.271235  0.865377 -2.062254  1.412337 -0.318887  1.570737  0.218967   \n",
       ":      3.070288  0.986614 -1.164513  1.810464  2.261192 -0.716290  0.249038   \n",
       ",      0.435715  2.059121 -2.827400  5.559331 -0.635631 -0.084876  1.348762   \n",
       ".      0.017206  1.433102  2.001210  3.140334  1.852441 -0.963332  1.899922   \n",
=======
<<<<<<< HEAD
       "\u0002     -0.917690  1.594071  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!      0.671140 -0.475780  1.225881 -0.533356  1.413613  2.528171 -0.030113   \n",
       "#      1.499199 -0.151667  2.150064  1.835209  1.904099  2.142190 -1.108657   \n",
       "$      0.630779  1.138583  2.530837  0.166182  3.076834  0.542186 -0.858886   \n",
       "\n",
       "            7         8         9      ...          374       375       376  \\\n",
       "<PAD>  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\u0002      2.663858  1.218638 -1.035808    ...     0.425276  1.230747  0.118247   \n",
       "!      0.486535  3.412096  1.299004    ...     0.361494  0.078374 -0.094767   \n",
       "#     -1.281632  2.732129  2.948514    ...     0.079698 -0.464940  1.290173   \n",
       "$      0.884038  2.754835 -0.390934    ...     0.297402 -0.254304  1.426802   \n",
       "\n",
       "            377       378       379       380       381       382       383  \n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\u0002     -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657523 -0.292479  \n",
       "!     -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#      0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476224  \n",
       "$     -0.010306 -0.657113 -0.627470  0.097198 -0.183204 -0.213610 -0.170229  \n",
=======
       "\u0002     -0.917688  1.594070  6.749008  0.536356 -1.675298  3.894897 -3.105777   \n",
       "!      0.671140 -0.475781  1.225882 -0.533356  1.413614  2.528172 -0.030113   \n",
       "#      1.499199 -0.151666  2.150062  1.835209  1.904099  2.142193 -1.108657   \n",
       "$      0.630779  1.138584  2.530838  0.166183  3.076835  0.542186 -0.858887   \n",
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "\n",
       "             7         8         9   ...        86        87        88  \\\n",
       "<PAD>  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       ">     -1.616881 -3.192619 -2.547622  ...  0.612776 -0.346440  4.841980   \n",
       ":     -1.720148  0.056825 -3.444734  ... -3.068707  0.864790  3.221823   \n",
       ",     -1.338799 -2.545363 -4.161990  ... -0.583077  0.877851  4.302979   \n",
       ".     -0.401818 -2.946477 -2.282340  ... -1.228596  0.986874  1.485204   \n",
       "\n",
       "             89        90        91        92        93        94        95  \n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
<<<<<<< HEAD
       ">      0.257593 -1.508441  5.375728  1.134497  0.064851 -0.028303  2.945163  \n",
       ":      1.004827 -0.291131  6.993183 -0.521152 -2.925200 -0.208749  2.764246  \n",
       ",     -2.965968 -0.829183  1.851194  4.129368 -2.011104 -1.491037  0.611928  \n",
       ".     -2.851583 -2.198703  2.439004  3.210545 -2.390744 -1.637757  0.658658  \n",
=======
       "\u0002     -0.219932  0.352065 -0.254422 -0.038656 -1.410054  0.657522 -0.292479  \n",
       "!     -0.087820 -0.176552  0.149058  0.224980 -0.329079  0.187947 -0.189483  \n",
       "#      0.061074 -0.257399 -0.752442  0.019620  0.132082 -0.440150 -0.476223  \n",
       "$     -0.010306 -0.657113 -0.627469  0.097199 -0.183204 -0.213610 -0.170229  \n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 10,
=======
<<<<<<< HEAD
     "execution_count": 32,
=======
     "execution_count": 11,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pd.DataFrame({'<PAD>': np.zeros(shape=[1,embed_words.shape[1]])[0]}).T\n",
    "embed_matrix = padding.append(embed_words)\n",
    "embed_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
<<<<<<< HEAD
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 13,
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   "outputs": [],
   "source": [
    "word_index = {j:i+1 for i,j in enumerate(embed_matrix.index.tolist()[1:])}\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = word_index\n",
    "sequences = tokenizer.texts_to_sequences(df.text)\n",
    "data = pad_sequences(sequences, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
<<<<<<< HEAD
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 14,
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
   "outputs": [],
   "source": [
    "random_matrix = np.random.randn(embed_matrix.shape[0], embed_matrix.shape[1])\n",
    "random_matrix[0] = np.zeros([1, embed_matrix.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 15,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodrigonader/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
<<<<<<< HEAD
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
=======
<<<<<<< HEAD
=======
       "       [0, 0, 0, ..., 0, 0, 0],\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
<<<<<<< HEAD
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
=======
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = pd.get_dummies(df['label'])\n",
    "target_labels = onehot.columns\n",
    "target = onehot.as_matrix()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
<<<<<<< HEAD
   "execution_count": 36,
=======
   "execution_count": 16,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.58047877, 0.57627204,\n",
       "        0.57627204],\n",
       "       [0.        , 0.        , 0.        , ..., 0.05719151, 0.48627804,\n",
       "        0.58047877],\n",
       "       [0.        , 0.        , 0.        , ..., 0.98532652, 0.58032853,\n",
       "        0.51527444],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.42698317, 0.78460537,\n",
       "        0.97105369],\n",
       "       [0.        , 0.        , 0.        , ..., 0.60957532, 0.61137821,\n",
       "        0.76872997],\n",
       "       [0.        , 0.        , 0.        , ..., 0.94265825, 0.78946314,\n",
       "        0.73207131]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(0.8 * len(data))\n",
    "\n",
    "x_train = data[:TRAIN_SIZE]\n",
    "x_test = data[TRAIN_SIZE:]\n",
    "\n",
    "y_train = target[:TRAIN_SIZE]\n",
    "y_test = target[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 23,
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 16:54:48.156697 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 16:54:48.197672 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 16:54:48.210897 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 16:54:48.228646 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0828 16:54:48.230071 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0828 16:54:48.367624 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0828 16:54:48.400146 4570703296 deprecation.py:506] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0828 16:54:48.472866 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 16:54:48.563578 4570703296 deprecation.py:323] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Train on 11520 samples, validate on 2880 samples\n",
      "Epoch 1/100\n",
      " - 11s - loss: 2.5865 - acc: 0.0752 - val_loss: 6.4346 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 10s - loss: 2.5688 - acc: 0.0752 - val_loss: 8.0693 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 10s - loss: 2.5655 - acc: 0.0765 - val_loss: 7.5530 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 10s - loss: 2.5652 - acc: 0.0752 - val_loss: 6.8016 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 10s - loss: 2.5641 - acc: 0.0819 - val_loss: 7.2698 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 10s - loss: 2.5641 - acc: 0.0730 - val_loss: 7.5392 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 10s - loss: 2.5637 - acc: 0.0773 - val_loss: 7.9959 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 10s - loss: 2.5635 - acc: 0.0747 - val_loss: 8.0127 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 10s - loss: 2.5629 - acc: 0.0751 - val_loss: 7.9182 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 10s - loss: 2.5626 - acc: 0.0834 - val_loss: 8.0354 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 10s - loss: 2.5625 - acc: 0.0772 - val_loss: 8.2547 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 10s - loss: 2.5622 - acc: 0.0737 - val_loss: 8.2440 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 10s - loss: 2.5623 - acc: 0.0757 - val_loss: 8.8726 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 10s - loss: 2.5618 - acc: 0.0793 - val_loss: 9.2227 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 10s - loss: 2.5624 - acc: 0.0733 - val_loss: 9.1702 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 10s - loss: 2.5619 - acc: 0.0759 - val_loss: 8.9990 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 10s - loss: 2.5618 - acc: 0.0769 - val_loss: 9.3051 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 10s - loss: 2.5620 - acc: 0.0759 - val_loss: 9.6033 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 10s - loss: 2.5619 - acc: 0.0772 - val_loss: 9.0790 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 10s - loss: 2.5615 - acc: 0.0819 - val_loss: 9.5364 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 10s - loss: 2.5616 - acc: 0.0751 - val_loss: 9.2177 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 10s - loss: 2.5615 - acc: 0.0740 - val_loss: 9.5981 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 10s - loss: 2.5613 - acc: 0.0785 - val_loss: 9.8618 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 10s - loss: 2.5616 - acc: 0.0770 - val_loss: 9.7845 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 10s - loss: 2.5614 - acc: 0.0735 - val_loss: 9.6992 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 10s - loss: 2.5614 - acc: 0.0766 - val_loss: 9.8440 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 10s - loss: 2.5612 - acc: 0.0768 - val_loss: 10.0069 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 10s - loss: 2.5613 - acc: 0.0790 - val_loss: 10.1457 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 10s - loss: 2.5614 - acc: 0.0724 - val_loss: 10.1919 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-5c734f2e4614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embed_matrix), len(embed_matrix.columns), input_length=MAXLEN, weights=[random_matrix],\n",
    "                           trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
=======
      "Train on 15514 samples, validate on 3879 samples\n",
      "Epoch 1/2\n",
      "15514/15514 [==============================] - 66s 4ms/step - loss: 2.3759 - acc: 0.2643 - val_loss: 0.9013 - val_acc: 0.6677\n",
      "Epoch 2/2\n",
      "15514/15514 [==============================] - 65s 4ms/step - loss: 0.4241 - acc: 0.8660 - val_loss: 0.1236 - val_acc: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1412e6080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embed_matrix), len(embed_matrix.columns), input_length=MAXLEN, weights=[embed_matrix],\n",
    "                           trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
>>>>>>> e7134105f41afe68ee253d9881e0383c164eed59
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(target.shape[1], activation='softmax')(x)\n",
    "model = Model(sequence_input, output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(data, target, validation_split=0.2, epochs=100, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0, 108],\n",
       "       [  0,   0,   0, ..., 150, 109, 101],\n",
       "       [  0,   0,   0, ..., 108, 109, 101],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  36,  75, 205],\n",
       "       [  0,   0,   0, ..., 107,  42,  20],\n",
       "       [  0,   0,   0, ...,   0,   1, 127]], dtype=int32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "collapsed": true
   },
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.3"
=======
   "version": "3.6.3"
>>>>>>> d4343a4f8b06f2d38fa93a3cf7338c66aa9ab47d
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
