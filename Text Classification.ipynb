{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from dalab import read_pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from time import time\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, SimpleRNN, GRU, LSTM\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17410</th>\n",
       "      <td>christian</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!rochester!rutg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4586</th>\n",
       "      <td>baseball</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9607</th>\n",
       "      <td>misc</td>\n",
       "      <td>Newsgroups: talk.politics.misc\\nPath: cantalou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>hardware</td>\n",
       "      <td>Path: cantaloupe.srv.cs.cmu.edu!rochester!udel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>med</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu misc.consumers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                               text\n",
       "17410  christian  Path: cantaloupe.srv.cs.cmu.edu!rochester!rutg...\n",
       "4586    baseball  Path: cantaloupe.srv.cs.cmu.edu!das-news.harva...\n",
       "9607        misc  Newsgroups: talk.politics.misc\\nPath: cantalou...\n",
       "2043    hardware  Path: cantaloupe.srv.cs.cmu.edu!rochester!udel...\n",
       "8281         med  Xref: cantaloupe.srv.cs.cmu.edu misc.consumers..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_pickle('data/20_newsgroup/dataframes/raw_news.pickle')\n",
    "df = df.sample(frac=1)\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('data/medium_stories/dataframes/en_lower_stories.pickle').reset_index(drop=True)\n",
    "# df = df.sample(frac=1)\n",
    "# df = df.drop_duplicates(subset='story')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian 997\n",
      "baseball 993\n",
      "misc 2681\n",
      "hardware 1965\n",
      "med 994\n",
      "x 991\n",
      "electronics 989\n",
      "guns 964\n",
      "space 991\n",
      "motorcycles 999\n",
      "mideast 973\n",
      "graphics 987\n",
      "forsale 992\n",
      "autos 990\n",
      "crypt 999\n",
      "hockey 994\n",
      "atheism 894\n"
     ]
    }
   ],
   "source": [
    "for topic in df['label'].unique():\n",
    "    print(topic, len(df[df['label'] == topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 1000\n",
    "VOCAB_SIZE = 20000\n",
    "TRAIN_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = word_tokenize(' '.join(df.text.tolist()))\n",
    "word_counts = Counter(all_words).most_common(VOCAB_SIZE)\n",
    "words = [w[0] for w in word_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n"
     ]
    }
   ],
   "source": [
    "embed_dic = {}\n",
    "for index, word in enumerate(words):\n",
    "    if index % 500 == 0: print(index)\n",
    "    token = nlp(word)\n",
    "    embed_dic[token.text] = token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&gt;</th>\n",
       "      <td>3.271235</td>\n",
       "      <td>0.865377</td>\n",
       "      <td>-2.062254</td>\n",
       "      <td>1.412337</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>1.570737</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>-1.616881</td>\n",
       "      <td>-3.192619</td>\n",
       "      <td>-2.547622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612776</td>\n",
       "      <td>-0.346440</td>\n",
       "      <td>4.841980</td>\n",
       "      <td>0.257593</td>\n",
       "      <td>-1.508441</td>\n",
       "      <td>5.375728</td>\n",
       "      <td>1.134497</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>2.945163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>3.070288</td>\n",
       "      <td>0.986614</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>1.810464</td>\n",
       "      <td>2.261192</td>\n",
       "      <td>-0.716290</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>-1.720148</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>-3.444734</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.068707</td>\n",
       "      <td>0.864790</td>\n",
       "      <td>3.221823</td>\n",
       "      <td>1.004827</td>\n",
       "      <td>-0.291131</td>\n",
       "      <td>6.993183</td>\n",
       "      <td>-0.521152</td>\n",
       "      <td>-2.925200</td>\n",
       "      <td>-0.208749</td>\n",
       "      <td>2.764246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.435715</td>\n",
       "      <td>2.059121</td>\n",
       "      <td>-2.827400</td>\n",
       "      <td>5.559331</td>\n",
       "      <td>-0.635631</td>\n",
       "      <td>-0.084876</td>\n",
       "      <td>1.348762</td>\n",
       "      <td>-1.338799</td>\n",
       "      <td>-2.545363</td>\n",
       "      <td>-4.161990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583077</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>4.302979</td>\n",
       "      <td>-2.965968</td>\n",
       "      <td>-0.829183</td>\n",
       "      <td>1.851194</td>\n",
       "      <td>4.129368</td>\n",
       "      <td>-2.011104</td>\n",
       "      <td>-1.491037</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.017206</td>\n",
       "      <td>1.433102</td>\n",
       "      <td>2.001210</td>\n",
       "      <td>3.140334</td>\n",
       "      <td>1.852441</td>\n",
       "      <td>-0.963332</td>\n",
       "      <td>1.899922</td>\n",
       "      <td>-0.401818</td>\n",
       "      <td>-2.946477</td>\n",
       "      <td>-2.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.228596</td>\n",
       "      <td>0.986874</td>\n",
       "      <td>1.485204</td>\n",
       "      <td>-2.851583</td>\n",
       "      <td>-2.198703</td>\n",
       "      <td>2.439004</td>\n",
       "      <td>3.210545</td>\n",
       "      <td>-2.390744</td>\n",
       "      <td>-1.637757</td>\n",
       "      <td>0.658658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>-0.438195</td>\n",
       "      <td>-0.151792</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>2.482028</td>\n",
       "      <td>0.040842</td>\n",
       "      <td>0.089026</td>\n",
       "      <td>2.102615</td>\n",
       "      <td>-1.106307</td>\n",
       "      <td>-3.831193</td>\n",
       "      <td>-2.613150</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.212205</td>\n",
       "      <td>0.977764</td>\n",
       "      <td>2.540429</td>\n",
       "      <td>-1.793072</td>\n",
       "      <td>-1.837151</td>\n",
       "      <td>3.268545</td>\n",
       "      <td>3.442772</td>\n",
       "      <td>-2.944798</td>\n",
       "      <td>-1.267171</td>\n",
       "      <td>0.232743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-1.553254</td>\n",
       "      <td>-2.913822</td>\n",
       "      <td>-4.319618</td>\n",
       "      <td>2.272106</td>\n",
       "      <td>-1.109262</td>\n",
       "      <td>1.648155</td>\n",
       "      <td>-2.616732</td>\n",
       "      <td>-2.518888</td>\n",
       "      <td>-2.483821</td>\n",
       "      <td>-2.783311</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250357</td>\n",
       "      <td>-1.727508</td>\n",
       "      <td>4.852344</td>\n",
       "      <td>-0.260655</td>\n",
       "      <td>3.556051</td>\n",
       "      <td>6.408743</td>\n",
       "      <td>-0.459191</td>\n",
       "      <td>1.640075</td>\n",
       "      <td>-0.309467</td>\n",
       "      <td>-1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--</th>\n",
       "      <td>-0.836566</td>\n",
       "      <td>1.029746</td>\n",
       "      <td>-1.545806</td>\n",
       "      <td>0.859156</td>\n",
       "      <td>3.777660</td>\n",
       "      <td>-0.975338</td>\n",
       "      <td>0.222508</td>\n",
       "      <td>-0.303487</td>\n",
       "      <td>-1.498767</td>\n",
       "      <td>-2.576878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.571487</td>\n",
       "      <td>1.666716</td>\n",
       "      <td>3.351637</td>\n",
       "      <td>1.144979</td>\n",
       "      <td>-1.691302</td>\n",
       "      <td>5.329454</td>\n",
       "      <td>0.757731</td>\n",
       "      <td>-3.216542</td>\n",
       "      <td>-1.397676</td>\n",
       "      <td>2.549656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@</th>\n",
       "      <td>1.050108</td>\n",
       "      <td>0.208937</td>\n",
       "      <td>0.347154</td>\n",
       "      <td>2.047950</td>\n",
       "      <td>1.639953</td>\n",
       "      <td>1.493342</td>\n",
       "      <td>1.208333</td>\n",
       "      <td>0.460561</td>\n",
       "      <td>-2.761452</td>\n",
       "      <td>-2.527010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.653786</td>\n",
       "      <td>0.561741</td>\n",
       "      <td>2.965085</td>\n",
       "      <td>1.466421</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>3.923660</td>\n",
       "      <td>4.240563</td>\n",
       "      <td>0.821820</td>\n",
       "      <td>-1.435084</td>\n",
       "      <td>3.934670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.061328</td>\n",
       "      <td>1.449228</td>\n",
       "      <td>-3.866241</td>\n",
       "      <td>0.745157</td>\n",
       "      <td>-1.463142</td>\n",
       "      <td>0.375383</td>\n",
       "      <td>-2.931649</td>\n",
       "      <td>-0.653668</td>\n",
       "      <td>-2.569345</td>\n",
       "      <td>-3.578854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.515135</td>\n",
       "      <td>2.122284</td>\n",
       "      <td>1.689541</td>\n",
       "      <td>3.462939</td>\n",
       "      <td>1.066636</td>\n",
       "      <td>7.533019</td>\n",
       "      <td>2.215232</td>\n",
       "      <td>-2.120562</td>\n",
       "      <td>-2.444374</td>\n",
       "      <td>2.538816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.479581</td>\n",
       "      <td>0.744241</td>\n",
       "      <td>-0.826027</td>\n",
       "      <td>0.408746</td>\n",
       "      <td>1.193886</td>\n",
       "      <td>-1.085895</td>\n",
       "      <td>-0.922773</td>\n",
       "      <td>-0.325466</td>\n",
       "      <td>-1.257374</td>\n",
       "      <td>-5.033413</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058636</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>3.694527</td>\n",
       "      <td>-1.901443</td>\n",
       "      <td>-2.631643</td>\n",
       "      <td>4.643446</td>\n",
       "      <td>3.615112</td>\n",
       "      <td>-3.458215</td>\n",
       "      <td>0.200469</td>\n",
       "      <td>3.789077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       ">    3.271235  0.865377 -2.062254  1.412337 -0.318887  1.570737  0.218967   \n",
       ":    3.070288  0.986614 -1.164513  1.810464  2.261192 -0.716290  0.249038   \n",
       ",    0.435715  2.059121 -2.827400  5.559331 -0.635631 -0.084876  1.348762   \n",
       ".    0.017206  1.433102  2.001210  3.140334  1.852441 -0.963332  1.899922   \n",
       "!   -0.438195 -0.151792  0.629396  2.482028  0.040842  0.089026  2.102615   \n",
       "the -1.553254 -2.913822 -4.319618  2.272106 -1.109262  1.648155 -2.616732   \n",
       "--  -0.836566  1.029746 -1.545806  0.859156  3.777660 -0.975338  0.222508   \n",
       "@    1.050108  0.208937  0.347154  2.047950  1.639953  1.493342  1.208333   \n",
       "to  -0.061328  1.449228 -3.866241  0.745157 -1.463142  0.375383 -2.931649   \n",
       ")    0.479581  0.744241 -0.826027  0.408746  1.193886 -1.085895 -0.922773   \n",
       "\n",
       "           7         8         9   ...        86        87        88  \\\n",
       ">   -1.616881 -3.192619 -2.547622  ...  0.612776 -0.346440  4.841980   \n",
       ":   -1.720148  0.056825 -3.444734  ... -3.068707  0.864790  3.221823   \n",
       ",   -1.338799 -2.545363 -4.161990  ... -0.583077  0.877851  4.302979   \n",
       ".   -0.401818 -2.946477 -2.282340  ... -1.228596  0.986874  1.485204   \n",
       "!   -1.106307 -3.831193 -2.613150  ... -2.212205  0.977764  2.540429   \n",
       "the -2.518888 -2.483821 -2.783311  ...  3.250357 -1.727508  4.852344   \n",
       "--  -0.303487 -1.498767 -2.576878  ... -2.571487  1.666716  3.351637   \n",
       "@    0.460561 -2.761452 -2.527010  ... -0.653786  0.561741  2.965085   \n",
       "to  -0.653668 -2.569345 -3.578854  ... -0.515135  2.122284  1.689541   \n",
       ")   -0.325466 -1.257374 -5.033413  ...  1.058636  2.931694  3.694527   \n",
       "\n",
       "           89        90        91        92        93        94        95  \n",
       ">    0.257593 -1.508441  5.375728  1.134497  0.064851 -0.028303  2.945163  \n",
       ":    1.004827 -0.291131  6.993183 -0.521152 -2.925200 -0.208749  2.764246  \n",
       ",   -2.965968 -0.829183  1.851194  4.129368 -2.011104 -1.491037  0.611928  \n",
       ".   -2.851583 -2.198703  2.439004  3.210545 -2.390744 -1.637757  0.658658  \n",
       "!   -1.793072 -1.837151  3.268545  3.442772 -2.944798 -1.267171  0.232743  \n",
       "the -0.260655  3.556051  6.408743 -0.459191  1.640075 -0.309467 -1.231600  \n",
       "--   1.144979 -1.691302  5.329454  0.757731 -3.216542 -1.397676  2.549656  \n",
       "@    1.466421  0.470562  3.923660  4.240563  0.821820 -1.435084  3.934670  \n",
       "to   3.462939  1.066636  7.533019  2.215232 -2.120562 -2.444374  2.538816  \n",
       ")   -1.901443 -2.631643  4.643446  3.615112 -3.458215  0.200469  3.789077  \n",
       "\n",
       "[10 rows x 96 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_words = pd.DataFrame(embed_dic).T\n",
    "embed_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;</th>\n",
       "      <td>3.271235</td>\n",
       "      <td>0.865377</td>\n",
       "      <td>-2.062254</td>\n",
       "      <td>1.412337</td>\n",
       "      <td>-0.318887</td>\n",
       "      <td>1.570737</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>-1.616881</td>\n",
       "      <td>-3.192619</td>\n",
       "      <td>-2.547622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612776</td>\n",
       "      <td>-0.346440</td>\n",
       "      <td>4.841980</td>\n",
       "      <td>0.257593</td>\n",
       "      <td>-1.508441</td>\n",
       "      <td>5.375728</td>\n",
       "      <td>1.134497</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>2.945163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>3.070288</td>\n",
       "      <td>0.986614</td>\n",
       "      <td>-1.164513</td>\n",
       "      <td>1.810464</td>\n",
       "      <td>2.261192</td>\n",
       "      <td>-0.716290</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>-1.720148</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>-3.444734</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.068707</td>\n",
       "      <td>0.864790</td>\n",
       "      <td>3.221823</td>\n",
       "      <td>1.004827</td>\n",
       "      <td>-0.291131</td>\n",
       "      <td>6.993183</td>\n",
       "      <td>-0.521152</td>\n",
       "      <td>-2.925200</td>\n",
       "      <td>-0.208749</td>\n",
       "      <td>2.764246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.435715</td>\n",
       "      <td>2.059121</td>\n",
       "      <td>-2.827400</td>\n",
       "      <td>5.559331</td>\n",
       "      <td>-0.635631</td>\n",
       "      <td>-0.084876</td>\n",
       "      <td>1.348762</td>\n",
       "      <td>-1.338799</td>\n",
       "      <td>-2.545363</td>\n",
       "      <td>-4.161990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.583077</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>4.302979</td>\n",
       "      <td>-2.965968</td>\n",
       "      <td>-0.829183</td>\n",
       "      <td>1.851194</td>\n",
       "      <td>4.129368</td>\n",
       "      <td>-2.011104</td>\n",
       "      <td>-1.491037</td>\n",
       "      <td>0.611928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.017206</td>\n",
       "      <td>1.433102</td>\n",
       "      <td>2.001210</td>\n",
       "      <td>3.140334</td>\n",
       "      <td>1.852441</td>\n",
       "      <td>-0.963332</td>\n",
       "      <td>1.899922</td>\n",
       "      <td>-0.401818</td>\n",
       "      <td>-2.946477</td>\n",
       "      <td>-2.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.228596</td>\n",
       "      <td>0.986874</td>\n",
       "      <td>1.485204</td>\n",
       "      <td>-2.851583</td>\n",
       "      <td>-2.198703</td>\n",
       "      <td>2.439004</td>\n",
       "      <td>3.210545</td>\n",
       "      <td>-2.390744</td>\n",
       "      <td>-1.637757</td>\n",
       "      <td>0.658658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       ">      3.271235  0.865377 -2.062254  1.412337 -0.318887  1.570737  0.218967   \n",
       ":      3.070288  0.986614 -1.164513  1.810464  2.261192 -0.716290  0.249038   \n",
       ",      0.435715  2.059121 -2.827400  5.559331 -0.635631 -0.084876  1.348762   \n",
       ".      0.017206  1.433102  2.001210  3.140334  1.852441 -0.963332  1.899922   \n",
       "\n",
       "             7         8         9   ...        86        87        88  \\\n",
       "<PAD>  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       ">     -1.616881 -3.192619 -2.547622  ...  0.612776 -0.346440  4.841980   \n",
       ":     -1.720148  0.056825 -3.444734  ... -3.068707  0.864790  3.221823   \n",
       ",     -1.338799 -2.545363 -4.161990  ... -0.583077  0.877851  4.302979   \n",
       ".     -0.401818 -2.946477 -2.282340  ... -1.228596  0.986874  1.485204   \n",
       "\n",
       "             89        90        91        92        93        94        95  \n",
       "<PAD>  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       ">      0.257593 -1.508441  5.375728  1.134497  0.064851 -0.028303  2.945163  \n",
       ":      1.004827 -0.291131  6.993183 -0.521152 -2.925200 -0.208749  2.764246  \n",
       ",     -2.965968 -0.829183  1.851194  4.129368 -2.011104 -1.491037  0.611928  \n",
       ".     -2.851583 -2.198703  2.439004  3.210545 -2.390744 -1.637757  0.658658  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = pd.DataFrame({'<PAD>': np.zeros(shape=[1,embed_words.shape[1]])[0]}).T\n",
    "embed_matrix = padding.append(embed_words)\n",
    "embed_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {j:i+1 for i,j in enumerate(embed_matrix.index.tolist()[1:])}\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = word_index\n",
    "sequences = tokenizer.texts_to_sequences(df.text)\n",
    "data = pad_sequences(sequences, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = np.random.randn(embed_matrix.shape[0], embed_matrix.shape[1])\n",
    "random_matrix[0] = np.zeros([1, embed_matrix.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rodrigonader/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = pd.get_dummies(df['label'])\n",
    "target_labels = onehot.columns\n",
    "target = onehot.as_matrix()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:TRAIN_SIZE]\n",
    "x_test = data[TRAIN_SIZE:]\n",
    "\n",
    "y_train = target[:TRAIN_SIZE]\n",
    "y_test = target[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 16:54:48.156697 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 16:54:48.197672 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 16:54:48.210897 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 16:54:48.228646 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0828 16:54:48.230071 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0828 16:54:48.367624 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0828 16:54:48.400146 4570703296 deprecation.py:506] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0828 16:54:48.472866 4570703296 deprecation_wrapper.py:119] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 16:54:48.563578 4570703296 deprecation.py:323] From /Users/rodrigonader/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15514 samples, validate on 3879 samples\n",
      "Epoch 1/2\n",
      "15514/15514 [==============================] - 66s 4ms/step - loss: 2.3759 - acc: 0.2643 - val_loss: 0.9013 - val_acc: 0.6677\n",
      "Epoch 2/2\n",
      "15514/15514 [==============================] - 65s 4ms/step - loss: 0.4241 - acc: 0.8660 - val_loss: 0.1236 - val_acc: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1412e6080>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(embed_matrix), len(embed_matrix.columns), input_length=MAXLEN, weights=[embed_matrix],\n",
    "                           trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(target.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, output)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "model.fit(data, target, validation_split=0.2, epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0, 108],\n",
       "       [  0,   0,   0, ..., 150, 109, 101],\n",
       "       [  0,   0,   0, ..., 108, 109, 101],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  36,  75, 205],\n",
       "       [  0,   0,   0, ..., 107,  42,  20],\n",
       "       [  0,   0,   0, ...,   0,   1, 127]], dtype=int32)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
